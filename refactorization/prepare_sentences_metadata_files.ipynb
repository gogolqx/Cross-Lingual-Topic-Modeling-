{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prepare_sentences_related_information_npy_files ",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "efybrcbJxrze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "53d350d3-c303-4d43-bc70-c71a39c7735f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ-9hjHFUUrs",
        "colab_type": "text"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhDVnir1GY9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "60f83e09-5269-43e8-f6c6-8f2acea538bd"
      },
      "source": [
        "%cd '/content/drive/My Drive/Colab Notebooks/opinion-lab-group-2.3/refactorization'\n",
        "%pwd "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/opinion-lab-group-2.3/refactorization\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/opinion-lab-group-2.3/refactorization'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn8ct1VtExpJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ba11e130-4100-4d8a-bb7f-e114e212899e"
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "from util import list2json"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWt0oCA4UZZF",
        "colab_type": "text"
      },
      "source": [
        "# Load global ID of desired pre-processed sentences\n",
        "i.e. Only sentences with length > 15, total number of desired pre-proceesed sentences is 328091."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXJII3A7yk7s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "871545b4-715b-46fe-c7e9-9205441f2810"
      },
      "source": [
        "directory = '/content/drive/My Drive/Colab Notebooks/opinion-lab-group-2.3/refactorization/data/'\n",
        "\n",
        "en_global_ids = np.load(directory + 'en_sentence_global_id.npy')\n",
        "de_global_ids = np.load(directory + 'de_sentence_global_id.npy')\n",
        "global_ids = np.concatenate((en_global_ids, de_global_ids))\n",
        "\n",
        "# choose k = 15 as optimal number of clusters from kmean clustering\n",
        "kmean_labels = np.load(directory + '15_kmean_labels.npy')\n",
        "\n",
        "en_kmean_labels = kmean_labels[:en_global_ids.shape[0]]\n",
        "de_kmean_labels = kmean_labels[en_global_ids.shape[0] : en_global_ids.shape[0] + de_global_ids.shape[0]]\n",
        "\n",
        "assert en_kmean_labels.shape == en_global_ids.shape\n",
        "assert de_kmean_labels.shape == de_global_ids.shape\n",
        "\n",
        "print('English global IDs: ', en_global_ids)\n",
        "print(type(en_global_ids), 'shape:', en_global_ids.shape)\n",
        "print()\n",
        "print('German global IDs:', de_global_ids)\n",
        "print(type(de_global_ids), 'shape:', de_global_ids.shape)\n",
        "print()\n",
        "print('kmean labels:', kmean_labels)\n",
        "print(type(kmean_labels), 'shape:', kmean_labels.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English global IDs:  [     0      1      2 ... 136422 136423 136424]\n",
            "<class 'numpy.ndarray'> shape: (127464,)\n",
            "\n",
            "German global IDs: [136425 136426 136427 ... 353783 353784 353785]\n",
            "<class 'numpy.ndarray'> shape: (200627,)\n",
            "\n",
            "kmean labels: [ 1  1  3 ... 12 10  7]\n",
            "<class 'numpy.ndarray'> shape: (328091,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKf6KJFYUfzC",
        "colab_type": "text"
      },
      "source": [
        "# Get related information for each pre-processed sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfhqI1DTAH6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fetch useful information from processed indexing file for all sentences\n",
        "with open(directory + 'all_sentences_index_with_date_cluster_senti.json', 'r') as f: \n",
        "  loaded_data = json.load(f)\n",
        "\n",
        "source, document_id, comment_id, date, sentiment = [], [], [], [], []\n",
        "# source, document_id, comment_id, date = [], [], [], []\n",
        "for item in loaded_data:\n",
        "  source.append(item['corpus_name'])\n",
        "  document_id.append(item['doc_id'])\n",
        "  comment_id.append(item['com_id'] if item['com_id'] != None else -1) # assign -1 to comment id for sentences belongs to articles inteads of comments\n",
        "  date.append(item['date'])\n",
        "  sentiment.append(item['sentiment'])\n",
        "\n",
        "# obtain small subset of information by lanuages for desired pre-processed sentences only\n",
        "en_source_sm = np.array(source)[en_global_ids]\n",
        "en_document_id_sm = np.array(document_id)[en_global_ids]\n",
        "en_comment_id_sm = np.array(comment_id)[en_global_ids]\n",
        "en_date_sm = np.array(date)[en_global_ids]\n",
        "en_sentiment_sm = np.array(sentiment)[en_global_ids]\n",
        "\n",
        "de_source_sm = np.array(source)[de_global_ids]\n",
        "de_document_id_sm = np.array(document_id)[de_global_ids]\n",
        "de_comment_id_sm = np.array(comment_id)[de_global_ids]\n",
        "de_date_sm = np.array(date)[de_global_ids]\n",
        "de_sentiment_sm = np.array(sentiment)[de_global_ids]\n",
        "\n",
        "assert en_global_ids.shape == en_source_sm.shape == en_document_id_sm.shape == en_comment_id_sm.shape == en_date_sm.shape # == en_sentiment_sm.shape\n",
        "assert de_global_ids.shape == de_source_sm.shape == de_document_id_sm.shape == de_comment_id_sm.shape == de_date_sm.shape # == de_sentiment_sm.shape\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1iCFwmzUTia",
        "colab_type": "text"
      },
      "source": [
        "# Save sentences related information as .npy files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSCMBhl-D5qn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c13daead-57e3-47de-ecfb-159d2e3c5b3f"
      },
      "source": [
        "np.save('en_sources.npy', en_source_sm)\n",
        "np.save('en_document_id.npy', en_document_id_sm)\n",
        "np.save('en_comment_id.npy', en_comment_id_sm)\n",
        "np.save('en_date.npy', en_date_sm)\n",
        "# np.save('en_sentiment.npy', en_sentiment_sm)\n",
        "\n",
        "np.save('de_sources.npy', de_source_sm)\n",
        "np.save('de_document_id.npy', de_document_id_sm)\n",
        "np.save('de_comment_id.npy', de_comment_id_sm)\n",
        "np.save('de_date.npy', de_date_sm)\n",
        "# np.save('de_sentiment.npy', de_sentiment_sm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/opinion-lab-group-2.3/refactorization/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frrm1MOqC-2H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "ca2d2ab0-4942-4ee6-cb5c-8a1d1dc8775a"
      },
      "source": [
        "# Simple random check to ensure the information matched\n",
        "directory = '/content/drive/My Drive/Colab Notebooks/opinion-lab-group-2.3/refactorization/data/'\n",
        "\n",
        "tmp_en_source_sm = np.load(directory + 'en_sources.npy')\n",
        "tmp_en_document_id_sm = np.load(directory + 'en_document_id.npy')\n",
        "tmp_en_comment_id_sm = np.load(directory + 'en_comment_id.npy')\n",
        "tmp_en_date_sm= np.load(directory + 'en_date.npy')\n",
        "# tmp_en_sentiment_sm = np.load(directory + 'en_sentiment.npy')\n",
        "\n",
        "tmp_de_source_sm = np.load(directory + 'de_sources.npy')\n",
        "tmp_de_document_id_sm = np.load(directory + 'de_document_id.npy')\n",
        "tmp_de_comment_id_sm = np.load(directory + 'de_comment_id.npy')\n",
        "tmp_de_date_sm = np.load(directory + 'de_date.npy')\n",
        "# tmp_de_sentiment_sm = np.load(directory + 'de_sentiment.npy')\n",
        "\n",
        "\n",
        "print('A random record from original dictionary:')\n",
        "print(loaded_data[353785])\n",
        "print()\n",
        "print('Corresponding records from smaller set of sentences, i.e. sentences with length <= 15 is ignored:')\n",
        "i = np.where(de_global_ids == 353785)[0]\n",
        "print('index:', i)\n",
        "print('global id:', de_global_ids[i])\n",
        "print('source:', tmp_de_source_sm[i])\n",
        "print('document id:', tmp_de_document_id_sm[i])\n",
        "print('comment_id:', tmp_de_comment_id_sm[i])\n",
        "print('date:', tmp_de_date_sm[i])\n",
        "# print('sentiment:', tmp_de_sentiment_sm[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A random record from original dictionary:\n",
            "{'global_id': 353785, 'corpus_name': 'spiegel', 'doc_id': 151, 'com_id': None, 'date': '2008-03-29', 'cluster': 11, 'sentiment': 0}\n",
            "\n",
            "Corresponding records from smaller set of sentences, i.e. sentences with length <= 15 is ignored:\n",
            "index: [200626]\n",
            "global id: [353785]\n",
            "source: ['spiegel']\n",
            "document id: [151]\n",
            "comment_id: [-1]\n",
            "date: ['2008-03-29']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8e70JZqB1Vr",
        "colab_type": "text"
      },
      "source": [
        "# Save sentences related information as .json files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBdUTCjCCAeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_dictionary = []\n",
        "for i in range(len(en_global_ids)):\n",
        "  list_of_dictionary.append({\n",
        "    'global_id': en_global_ids[i],\n",
        "    'corpus_name': en_source_sm[i],\n",
        "    'doc_id': en_document_id_sm[i], \n",
        "    'com_id': en_comment_id_sm[i], \n",
        "    'date': en_date_sm[i], \n",
        "    'cluster': en_kmean_labels[i], \n",
        "    'sentiment': en_sentiment_sm[i]\n",
        "  })\n",
        "\n",
        "for i in range(len(de_global_ids)):\n",
        "  list_of_dictionary.append({\n",
        "    'global_id': de_global_ids[i],\n",
        "    'corpus_name': de_source_sm[i],\n",
        "    'doc_id': de_document_id_sm[i], \n",
        "    'com_id': de_comment_id_sm[i], \n",
        "    'date': de_date_sm[i], \n",
        "    'cluster': de_kmean_labels[i], \n",
        "    'sentiment': de_sentiment_sm[i]\n",
        "  })"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1rSLHhQJMSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list2json(list_of_dictionary, 'sentence_cluster_sentiment_dict.json')"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}